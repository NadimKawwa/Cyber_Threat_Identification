from time import sleep
from pymongo import MongoClient
from FakePersona import getPage


base_url = "https://www.exploit-db.com"

def getExploitCategories():
    #access page as fake persona
    soup = getPage(base_url)
    #find all list items <li>
    categories = soup.find("ul", {"class":"w-nav-list"}).findAll("li", {"class":"level_1"})[1]
    #get anchor
    categories = [i.find("a")['href'] for i in categories.findAll("li")]
    return categories


def getCategoryTable(pageSoup):
    table = pageSoup.find("table", {"class": "exploit_list"}).findAll("tr")
    return table

def streamData(data,collection):
    client = MongoClient(host='localhost', port=27017)
    db = client.exploits
    db[collection].insert(data)


def streamExploitTableSoup(tableSoup, category, database):
    for i in tableSoup:
        try:

            rows = i.findAll("td")
            rows = rows[0:1] + rows[3::]

            if len(rows) == 5:
                date = rows[0].getText()
                verification = rows[1].find("img")['title'].strip()
                exploitLink = rows[2].find("a")
                title,link= exploitLink.getText().replace("\n","").strip(), exploitLink['href']

                if "-" in title:
                    appattack = title.split("-")
                    application = appattack[0]
                    attack = appattack[1]
                else:
                    application = title
                    attack = title
                platform = rows[3].find("a").getText().replace("\n","").strip()
                author = rows[4].find("a").getText().replace("\n","").strip()

                #datetime = date.split("-")
                #data = {"date":{"fulldate":date,"year":datetime[0], "month":datetime[1], "day":datetime[2]},
                 #       "attack":{"application":application,"vector":attack},
                  #      "platform":platform, "author":author, "link":link, "verification":verification}
                #streamData(data=data, collection=database)

                print("{0},{1},{2},{3},{4},{5}".format(date, application, attack,platform, author, verification, link))
        except:
            pass

def crawlCategoryTables(categoryLink):
    category = categoryLink.split("/")[-2]
    categoryPage =getPage(categoryLink)
    lastPage = int(categoryPage.find("main").find("div", {"class":"pagination"}).findAll("a")[-1]['href'].split("=")[-1])
    for i in range(1,(lastPage+1)):
        if i % 20 == 0:
            sleep(60)
        newUrl = categoryLink + "?order_by=date_published&order=desc&pg="+str(i)
        ts = getCategoryTable(getPage(newUrl))
        streamExploitTableSoup(ts, category = category, database =category)


if __name__  == "__main__":
    exploitCategories = getExploitCategories()
    crawlCategoryTables(exploitCategories[3])
    
    
    
    
    
    
    
    
